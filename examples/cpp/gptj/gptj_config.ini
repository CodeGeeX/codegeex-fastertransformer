[ft_instance_hyperparameter]
max_batch_size=8 ; Use for allocate the buffer
max_seq_len=128 ; The sequence length of position embedding table, should move to model hyper-parameter
beam_width=1 ; beam width for beam search
top_k=0 ; k value for top k sampling
top_p=0.5 ; p value for top p sampling
temperature=1.0 ; Use for sampling
repetition_penalty=2.0 ; Use for sampling
len_penalty=1.0
beam_search_diversity_rate=0.0
is_half=0
enable_custom_all_reduce=0

tensor_para_size=8
pipeline_para_size=1

model_name=gptj_6B
model_dir=../models/j6b_ckpt/

[request]
request_batch_size=8 # determine by the request
request_output_len=32 # determine by the request

[gptj_6B]
head_num=16
size_per_head=256
vocab_size=50400
decoder_layers=28
rotary_embedding=64
start_id=50256
end_id=50256
inter_size=16384
